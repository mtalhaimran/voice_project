import os
import io
import time
import numpy as np
import streamlit as st
from pypdf import PdfReader
from openai import OpenAI

# ---------- Config ----------
st.set_page_config(page_title="Edu Voice MVP (Text Demo)", page_icon="ðŸŽ“", layout="centered")

# ---------- Helpers ----------
def get_api_key():
    # Prefer env var; allow UI override for quick tests
    k_env = os.getenv("OPENAI_API_KEY", "").strip()
    k_ui = st.session_state.get("api_key_input", "").strip()
    return k_ui or k_env

def read_txt(file):
    return file.read().decode("utf-8", errors="ignore")

def read_pdf(file):
    reader = PdfReader(file)
    parts = []
    for page in reader.pages:
        text = page.extract_text() or ""
        parts.append(text)
    return "\n".join(parts)

def chunk_text(text, max_tokens_est=400):
    # naive chunker by characters. ~4 chars â‰ˆ 1 token. 400 tokens â‰ˆ 1600 chars.
    chunk_size = 1600
    overlap = 200
    text = text.strip().replace("\r", "")
    chunks = []
    i = 0
    while i < len(text):
        chunk = text[i : i + chunk_size]
        chunks.append(chunk)
        i += chunk_size - overlap
    return chunks

def embed_texts(client, texts):
    # Use a small, cheap embedding model
    # text-embedding-3-small returns 1536-d vectors
    res = client.embeddings.create(model="text-embedding-3-small", input=texts)
    return np.array([d.embedding for d in res.data], dtype=np.float32)

def cosine_sim(a, b):
    a_norm = a / (np.linalg.norm(a) + 1e-8)
    b_norm = b / (np.linalg.norm(b, axis=1, keepdims=True) + 1e-8)
    return np.dot(b_norm, a_norm)

def retrieve_context(client, kb_chunks, kb_embeds, query, top_k=3):
    if not kb_chunks:
        return ""
    q_vec = embed_texts(client, [query])[0]
    sims = cosine_sim(q_vec, kb_embeds)
    idx = np.argsort(-sims)[:top_k]
    top_chunks = [kb_chunks[i] for i in idx]
    return "\n\n".join(top_chunks)

def ask_llm(client, model, system, user, max_tokens=400, temperature=0.3):
    resp = client.chat.completions.create(
        model=model,
        messages=[
            {"role":"system","content":system},
            {"role":"user","content":user}
        ],
        temperature=temperature,
        max_tokens=max_tokens,
        stream=True
    )
    # Stream tokens to UI
    full = ""
    for chunk in resp:
        delta = chunk.choices[0].delta.content or ""
        full += delta
        yield delta
    # small delay to ensure UI flush
    time.sleep(0.05)

# ---------- UI ----------
st.title("ðŸŽ“ Educational AI â€“ MVP (Text Q&A)")
st.caption("Paste/upload a small dataset, ask a question, get an answer. Model: gpt-4o-mini by default.")

with st.sidebar:
    st.subheader("Settings")
    st.text_input("OpenAI API Key", type="password", key="api_key_input",
                  help="Prefer using environment variable OPENAI_API_KEY in production.")
    model = st.selectbox("Model", ["gpt-4o-mini", "gpt-4o"], index=0)
    tone = st.selectbox("AI Tone / Role", ["Friendly Tutor", "Formal Academic", "Kid-Friendly", "Concise Assistant"], index=0)
    top_k = st.slider("Contexts to retrieve", 1, 5, 3)
    st.markdown("---")
    st.caption("Upload a small PDF or TXT (a few pages is fine for MVP).")

    uploaded = st.file_uploader("Upload knowledge (.pdf or .txt)", type=["pdf","txt"])
    pasted = st.text_area("Or paste notes here", height=180, placeholder="Paste a short chapter, bullet notes, or definitions...")

api_key = get_api_key()
if not api_key:
    st.warning("Add your OpenAI API key in the sidebar (or set OPENAI_API_KEY env var) to begin.")
    st.stop()

client = OpenAI(api_key=api_key)

# Build knowledge base (KB)
kb_texts = []
if pasted and pasted.strip():
    kb_texts.append(pasted.strip())

if uploaded is not None:
    try:
        if uploaded.name.lower().endswith(".pdf"):
            kb_texts.append(read_pdf(uploaded))
        else:
            kb_texts.append(read_txt(uploaded))
    except Exception as e:
        st.error(f"Failed to read file: {e}")

kb_text = "\n\n".join([t for t in kb_texts if t])
kb_chunks, kb_embeds = [], None

if kb_text:
    kb_chunks = chunk_text(kb_text)
    try:
        kb_embeds = embed_texts(client, kb_chunks)
    except Exception as e:
        st.error(f"Embedding error: {e}")
        kb_chunks = []
        kb_embeds = None

# Chat area
st.markdown("### Ask a question")
question = st.text_input("Your question", placeholder="e.g., Explain photosynthesis in simple steps.")

col1, col2 = st.columns([1,1])
with col1:
    go = st.button("Get Answer")
with col2:
    clear = st.button("Clear")

if clear:
    st.experimental_rerun()

# System prompts
tone_map = {
    "Friendly Tutor": "You are a friendly, supportive tutor. Explain clearly, step by step, with simple language and examples.",
    "Formal Academic": "You are a formal academic instructor. Provide rigorous, structured explanations with definitions and references if useful.",
    "Kid-Friendly": "You are a patient teacher speaking to a 10-year-old. Use simple words, short sentences, and analogies.",
    "Concise Assistant": "Answer concisely. Use bullet points when helpful. Avoid unnecessary detail."
}
system_prompt = tone_map[tone]

if go:
    if not question.strip():
        st.warning("Please enter a question.")
        st.stop()

    # Build context
    context = ""
    if kb_chunks and kb_embeds is not None:
        context = retrieve_context(client, kb_chunks, kb_embeds, question, top_k=top_k)

    user_prompt = (
        f"Answer the question using the context if relevant.\n\n"
        f"---\nContext:\n{context}\n---\n\nQuestion: {question}"
        if context else f"Question: {question}"
    )

    st.markdown("**Answer:**")
    placeholder = st.empty()
    collected = ""
    try:
        for token in ask_llm(client, model=model, system=system_prompt, user=user_prompt):
            collected += token
            placeholder.markdown(collected)
    except Exception as e:
        st.error(f"LLM error: {e}")

st.markdown("---")
st.caption("MVP: text Q&A with optional context from your uploads/notes. Voice I/O can be added in the next iteration.")
